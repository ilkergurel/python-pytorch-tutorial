{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  tensor([[ 85.3284,  79.8175],\n",
      "        [ 94.2271, 108.3222],\n",
      "        [ 82.0088,  40.1266],\n",
      "        [155.6003, 140.4139],\n",
      "        [ 41.7183,  76.4102]], grad_fn=<AddBackward0>)\n",
      "Targets (compare with predictions) tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "loss at 1st iteration:  tensor(4529.1719, grad_fn=<DivBackward0>)\n",
      "w at 1st iteration:  tensor([[ 2.0112, -0.2303, -1.0699],\n",
      "        [ 1.3884, -1.1853,  1.3526]], requires_grad=True)\n",
      "dLoss/dw = w.grad at 1st iteration:  tensor([[ 1905.0392,  -393.2123,   123.1316],\n",
      "        [  182.5052, -2156.9475,  -730.1721]])\n",
      "loss at final iteration:  tensor(1318.9180, grad_fn=<DivBackward0>)\n",
      "Predictions:  tensor([[ 69.4486,  80.2073],\n",
      "        [ 79.9534, 113.4172],\n",
      "        [104.0704,  88.0539],\n",
      "        [ 91.4740,  95.4356],\n",
      "        [ 56.9091, 107.1648]], grad_fn=<AddBackward0>)\n",
      "Targets:  tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "#PYTORCH LINEAR REGRESSION EXAMPLE\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "inputs = np.array([[73, 67, 43],[91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70]], dtype='float32')\n",
    "inputs = torch.from_numpy(inputs)\n",
    "\n",
    "targets = np.array([[56, 70],[81, 101], [119, 133], [22, 37], [103, 119]], dtype='float32')\n",
    "targets = torch.from_numpy(targets)\n",
    "\n",
    "#weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "\n",
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "#Generate predictions\n",
    "preds = model(inputs)\n",
    "print(\"Predictions: \", preds)\n",
    "print(\"Targets (compare with predictions)\", targets)\n",
    "\n",
    "#Loss function -- MSE loss\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "\n",
    "#Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(\"loss at 1st iteration: \", loss)\n",
    "\n",
    "#Compute gradients\n",
    "loss.backward()\n",
    "print(\"w at 1st iteration: \", w)\n",
    "print(\"dLoss/dw = w.grad at 1st iteration: \", w.grad)\n",
    "\n",
    "# Train for 100 epochs: Adjust weights & reset gradients\n",
    "for i in range(100):\n",
    "    #Compute predictions\n",
    "    preds = model(inputs)\n",
    "\n",
    "    #Compute loss\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #Apply gradient descend algorithm\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        \n",
    "        #Reset gradients to zero\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "\n",
    "#Compute loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(\"loss at final iteration: \", loss)\n",
    "\n",
    "\n",
    "print(\"Predictions: \", preds)\n",
    "print(\"Targets: \", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb:  tensor([[ 91.,  88.,  64.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "yb:  tensor([[ 81., 101.],\n",
      "        [ 22.,  37.],\n",
      "        [ 56.,  70.],\n",
      "        [119., 133.],\n",
      "        [103., 119.]])\n",
      "model.weight:  Parameter containing:\n",
      "tensor([[ 0.0726, -0.3803,  0.1251],\n",
      "        [-0.2994, -0.4127,  0.0879]], requires_grad=True)\n",
      "model.bias:  Parameter containing:\n",
      "tensor([-0.2668,  0.5632], requires_grad=True)\n",
      " Epoch [10/100], Loss: 1101.4525\n",
      " Epoch [20/100], Loss: 557.0427\n",
      " Epoch [30/100], Loss: 483.2258\n",
      " Epoch [40/100], Loss: 426.0412\n",
      " Epoch [50/100], Loss: 375.8216\n",
      " Epoch [60/100], Loss: 331.5875\n",
      " Epoch [70/100], Loss: 292.6219\n",
      " Epoch [80/100], Loss: 258.2962\n",
      " Epoch [90/100], Loss: 228.0571\n",
      " Epoch [100/100], Loss: 201.4169\n",
      "Predictions:  tensor([[ 62.2603,  74.2141],\n",
      "        [ 85.0206, 101.4307],\n",
      "        [104.1019, 124.9525],\n",
      "        [ 50.8888,  58.9692],\n",
      "        [ 89.3687, 107.7617]], grad_fn=<AddmmBackward0>)\n",
      "Targets:  tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression using Pytorch built-ins\n",
    "#Run previous step\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "\n",
    "#Define data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "for xb, yb in train_dl:\n",
    "    print(\"xb: \", xb)\n",
    "    print(\"yb: \", yb)\n",
    "    break\n",
    "\n",
    "#Define model\n",
    "\n",
    "model = nn.Linear(3,2)\n",
    "print(\"model.weight: \", model.weight)\n",
    "print(\"model.bias: \", model.bias)\n",
    "\n",
    "#Parameters\n",
    "list(model.parameters())\n",
    "\n",
    "#Generate predictions\n",
    "\n",
    "preds = model(inputs)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "#Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "#Train the model\n",
    "\n",
    "#Utility function to train the model\n",
    "\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    for epoch in range(num_epochs):\n",
    "        #Train with batches of data\n",
    "        for xb, yb in train_dl:\n",
    "            #Generate predictions\n",
    "            pred = model(xb)\n",
    "\n",
    "            #Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            #Compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Update parameters using gradients\n",
    "            opt.step()\n",
    "\n",
    "            #Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        #Print progress\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(' Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss))\n",
    "\n",
    "fit(100, model, loss_fn, opt, train_dl)\n",
    "\n",
    "#Generate predictions\n",
    "preds = model(inputs)\n",
    "print(\"Predictions: \", preds)\n",
    "print(\"Targets: \", targets)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
